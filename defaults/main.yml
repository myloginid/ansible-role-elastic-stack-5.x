---

# java stuff
install_java: no


# elastic general stuff
elastic_major_version: "5.x"
elastic_apt_key_url: "https://artifacts.elastic.co/GPG-KEY-elasticsearch"
elastic_apt_sources_deb: "deb https://artifacts.elastic.co/packages/{{ elastic_major_version }}/apt stable main"
elastic_apt_sources_list_path: "/etc/apt/sources.list.d/elastic-{{ elastic_major_version }}.list"


# elasticsearch stuff
install_elasticsearch: no
es_version: 5.1.1
es_heap_size: 512m

es_yml:
  cluster.name: elasticsearch
  node.name: "{{ ansible_hostname }}"
  node.attr.rack: ~
  path.data: /var/lib/elasticsearch
  path.logs: /var/log/elasticsearch
  bootstrap.memory_lock: 1
  network.host: 127.0.0.1
  http.port: 9200
  discovery.zen.ping.unicast.hosts: ~
  discovery.zen.minimum_master_nodes: ~
  gateway.recover_after_nodes: ~
  action.destructive_requires_name: 1

es_jvm_options:
  min_heap_size: "{{ es_heap_size }}"
  max_heap_size: "{{ es_heap_size }}"


# logstash stuff
install_logstash: no
ls_version: 5.1.1-1
ls_min_heap_size: 512m
ls_max_heap_size: 1g
ls_confd_files: []

ls_yml:
  node.name: "{{ ansible_hostname }}"
  path.data: /var/lib/logstash
  pipeline.workers: "{{ ansible_processor_count }}"
  pipeline.output.workers: 1
  pipeline.batch.size: 125
  pipeline.batch.delay: 5
  pipeline.unsafe_shutdown: "false"
  path.config: /etc/logstash/conf.d
  config.string: ~
  config.test_and_exit: "false"
  config.reload.automatic: "false"
  config.reload.interval: 3
  config.debug: "false"
  queue.type: memory
  path.queue: ~
  queue.page_capacity: 250mb
  queue.max_events: 0
  queue.max_bytes: 1024mb
  queue.checkpoint.acks: 1024
  queue.checkpoint.writes: 1024
  queue.checkpoint.interval: 1000
  http.host: "127.0.0.1"
  http.port: "9600-9700"
  log.level: info
  path.logs: /var/log/logstash
  path.plugins: []

ls_startup_opts:
  java_opts: "-Xms{{ ls_min_heap_size }} -Xmx{{ ls_max_heap_size }} -XX:MaxPermSize"

ls_jvm_options:
  min_heap_size: "{{ ls_min_heap_size }}"
  max_heap_size: "{{ ls_max_heap_size }}"


# filebeat stuff
install_filebeat: no

fb_full_yml:
  # prospectors
  filebeat.prospectors:
    - input_type: log
      # control values
      prospector_is_json: no
      prospector_expects_multiline: no
      # config values
      paths:
        - /var/log/*.log
      encoding: utf-8
      exclude_lines:
        - ^DBG
      include_lines:
        - ^ERR
        - ^WARN
      exclude_files:
        - .gz$
      fields_under_root: "false"
      ignore_older: "0"
      document_type: log
      scan_frequency: 10s
      harvester_buffer_size: 16384
      max_bytes: 10485760
      json.message_key:
      json.keys_under_root: "false"
      json.overwrite_keys: "false"
      json.add_error_key: "false"
      multiline.pattern: ^\[
      multiline.negate: "false"
      multiline.match: after
      multiline.max_lines: 500
      multiline.timeout: 5s
      tail_files: "false"
      symlinks: "false"
      backoff: 1s
      max_backoff: 10s
      backoff_factor: 2
      harvester_limit: 0
      close_inactive: 5m
      close_renamed: "false"
      close_removed: "true"
      close_eof: "false"
      clean_inactive: 0
      clean_removed: "true"
      close_timeout: 0
  # global config
  filebeat.spool_size: 2048
  filebeat.publish_async: "false"
  filebeat.idle_timeout: 5s
  filebeat.registry_file: ~
  filebeat.config_dir: ~
  filebeat.shutdown_timeout: 0
  # shipper config
  name: ~
  tags: ~
  fields: ~
  fields_under_root: ~
  queue_size: ~
  bulk_queue_size: ~
  max_procs: ~
  # output configs
  output.elasticsearch:
    enabled: no
    hosts:
      - localhost:9200
    compression_level: 0
    protocol: ~
    username: ~
    password: ~
    parameters: ~
    worker: 1
    index: "{{ client }}"  # `client` is required user defined
    pipeline: ~
    path: ~
    proxy_url: ~
    max_retries: 3
    bulk_max_retries: 50
    timeout: 90
    flush_interval: 1s
    template.enabled: "true"
    template.name: "filebeat"
    template.path: "/etc/filebeat/filebeat.template.json"
    template.overwrite: "false"
    ssl.enabled: "false"
  output.logstash:
    enabled: no
    hosts:
      - localhost:5044
    worker: 1
    compression_level: 3
    loadbalance: "true"
    pipelining: 0
    index: "{{ client }}"  # `client` is required user defined
    proxy_url: ~
    proxy_use_local_resolver: "false"
    ssl.enabled: no
  output.kafka:
    enabled: no
    hosts:
      - localhost:9092
    topic: beats
    key: "{{ client }}"
    worker: 1
    max_retries: 3
    bulk_max_size: 2048
    timeout: 30s
    broker_timeout: 10s
    channel_buffer_size: 256
    keep_alive: 0
    compression: gzip
    max_message_bytes: 1000000
    required_acks: 1
    flush_interval: 1s
    ssl.enabled: no
  output.file:
    enabled: no
    path: /tmp/filebeat
    filename: filebeat
    rotate_every_kb: 10000
    number_of_files: 7
  output.console:
    enabled: no
    pretty: "false"
  # path info
  path.home: /usr/share/filebeat
  path.config: /etc/logstash
  path.data: /var/lib/filebeat
  path.logs: /var/log/filebeat
  # logging
  logging.level: info
  logging.metrics.enabled: "true"
  logging.metrics.period: 30s
  logging.to_files: "true"


# kibana stuff
install_kibana: no
